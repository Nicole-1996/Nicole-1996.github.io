<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>YOLO</title>
    <url>/2023/09/27/YOLO/</url>
    <content><![CDATA[<h3 id="论文信息">论文信息</h3>
<p>标题：You Only Look Once: Unified, Real-Time Object Detection<br />
会议：CVPR 2016<br />
作者：Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi<br />
单位：University of Washington, Allen Institute for AI, Facebook AI
Research<br />
论文链接：<a
href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">YOLO</a><br />
项目链接：<a href="https://pjreddie.com/darknet/yolov1/">YOLO: Real-Time
Object Detection</a></p>
<hr />
<h3 id="motivation">Motivation</h3>
<p>此前取得较好结果的物体检测方法大多是多阶段的，这些方法比较复杂、速度慢而且难以单独优化。因此
YOLO 提出了一个 end-to-end
的单阶段物体检测方法，用于提升实时物体检测的性能。</p>
<hr />
<h3 id="优缺点">优缺点</h3>
<h4 id="优点">优点</h4>
<ol type="1">
<li>速度快。可以实时处理视频流，且精度远优于其他实时物体检测算法；</li>
<li>误检的背景比较少。作者认为主要原因是 YOLO 没有采取划窗或 proposal
的方式，使得模型能利用更多上下文信息；</li>
<li>泛化性好。</li>
</ol>
<h4 id="缺点">缺点</h4>
<ol type="1">
<li>精度无法与最好的物体检测方法相比，精准定位能力较差，尤其对于小物体定位效果更差；</li>
<li>在 YOLO 的设计框架下，无法对成群的小物体进行检测。</li>
</ol>
<hr />
<h3 id="method">Method</h3>
<h4 id="整体思路">整体思路</h4>
<ol type="1">
<li>整个图片被划分为 <span class="math inline">\(S\times S\)</span>
的网格，如果物体的中心落在某个网格里，则由该网格负责预测这个物体；</li>
<li>每个格子预测 <span class="math inline">\(B\)</span> 个 bounding box
以及每个 box 的置信分数。YOLO 里面的置信分数不单是 bounding box
里包含物体的概率，而是被定义为 <span
class="math inline">\(Pr(Object)\times
IOU_{pred}^{truth}\)</span>，即当格子里没有物体时，置信分数为
0，当格子里有物体时，希望得到的置信分数是检测框和 gound truth 之间的
IOU；</li>
<li>对于每个 bounding box，预测值有 5 个，分别是表示检测框位置的 <span
class="math inline">\(x, y, w, h\)</span> 以及置信分数；</li>
<li>除了 <span class="math inline">\(B\)</span> 个 bounding box
的信息以外，每个格子还预测当前格子对应的物体类别；<span
class="math inline">\(Pr
(Class_i|Object)\)</span>。需要注意的是类别预测是每个格子预测一组，而不是每个
bounding box 预测一组；</li>
<li>测试时只需要把类别概率与每个 bounding box
的置信分数相乘，就可以得到每个 bounding box 在每个类别上的分数。
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-09-28%2013.03.04.png" width="70%" height="70%" /></li>
</ol>
<h4 id="network">Network</h4>
<p>模型网络采用了 GoogleNet。 <img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-09-28%2013.43.35.png"
alt="截屏2023-09-28 13.43.35.png" /></p>
<h4 id="loss-设计">Loss 设计</h4>
<p><span class="math display">\[
\begin{align}
Loss&amp;=\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2]\\
&amp;+\lambda_{coord}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}[(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2]\\
&amp;+\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2\\
&amp;+\lambda_{noobj}\sum_{i=0}^{S^2}\sum_{j=0}^{B}\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2\\
&amp;+\sum_{i=0}^{S^2}\mathbb{1}_{i}^{obj}\sum_{c\in
classes}(p_i(c)-\hat{p}_i(c))^2
\end{align}
\]</span> 1. 实验中设置 <span class="math inline">\(\lambda_{coord}=5,
\lambda_{noobj}=0.5\)</span>
是为了稳定训练，因为物体训练过程中大部分格子都没有物体，如果不设置权重，第四项
loss 会主导训练，使模型变得不稳定； 2. 第 2 项 loss 中用 box
宽、高的平方根相减计算误差，而非直接用宽、高的原始数值计算误差，是为了缓解大小物体之间的差异。同样的误差在大物体上远没有小物体上严重，因此
YOLO 采用平方根的方式缓解这种差异。</p>
<hr />
<h3 id="实验结果">实验结果</h3>
<h4 id="与其他-real-time-检测方法比较">与其他 real time
检测方法比较</h4>
<ol type="1">
<li>YOLO 检测效果远高于之前的 real time 检测方法；</li>
<li>检测性能低于非 real time 的检测方法。
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-07%2013.31.59.png" width="70%" height="70%" /></li>
</ol>
<h4 id="yolo-与-faster-r-cnn-的错误分析">YOLO 与 Faster R-CNN
的错误分析</h4>
<p>文章中定义了 5 种不同的检测结果，并分析了 YOLO 和 Faster R-CNN
的检测结果中每种检测结果的占比： 1. 正确检出：<span
class="math inline">\(IOU&gt;0.5\)</span> 且类别正确； 2.
定位错误：<span class="math inline">\(0.1&lt;IOU&lt;0.5\)</span>
且类别正确； 3. 相似：<span class="math inline">\(IOU&gt;0.1\)</span>
且类别相似； 4. 其他错误：<span
class="math inline">\(IOU&gt;0.1\)</span> 且类别错误； 5.
检出背景：<span class="math inline">\(IOU&lt;0.1\)</span>。</p>
<p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-07%2013.41.11.png" width="70%" height="70%" />
可以看出，与 Faster R-CNN 相比，YOLO
检出的纯背景比例大幅减少，但定位错误比例更高。</p>
<h4 id="在文艺作品数据集上">在文艺作品数据集上</h4>
<p>YOLO
的泛化性优于其他方法，即使在风格差距比较大的数据集上也能保持不错的检测结果。
<img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-07%2014.04.03.png"
alt="截屏2023-10-07 14.04.03.png" /></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>论文解读</category>
        <category>Object Detection</category>
        <category>YOLO系列</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>YOLO</tag>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv2 (YOLO9000)</title>
    <url>/2023/10/07/YOLOv2%20(YOLO9000)/</url>
    <content><![CDATA[<h3 id="论文信息">论文信息</h3>
<p>标题：YOLO 9000: Better, Faster, Stronger<br />
会议：CVPR 2017<br />
作者：Joseph Redmon, Ali Farhadi<br />
单位：University of Washington, Allen Institute for AI, XNOR. Ai<br />
论文链接：<a
href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf">YOLO9000</a><br />
项目链接：<a href="https://pjreddie.com/darknet/yolov2/">YOLO: Real-Time
Object Detection</a></p>
<hr />
<h3 id="本文工作">本文工作</h3>
<ol type="1">
<li>对 YOLO 框架进行改进，加入一些 trick 和调整，提升算法性能得到
YOLOv2；</li>
<li>利用 ImageNet 数据集辅助训练，增加模型可以检出的类别数。</li>
</ol>
<hr />
<h3 id="提升模型性能">提升模型性能</h3>
<h4 id="加入-bn-层">加入 BN 层</h4>
<p>提升性能和收敛速度，防止过拟合。</p>
<h4 id="high-resolution-classifier">High Resolution Classifier</h4>
<p>YOLO 之前的训练方式是在 <span class="math inline">\(224\times
224\)</span> 的数据上用分类任务对模型预训练，然后迁移到 <span
class="math inline">\(448\times 448\)</span>
图像的检测问题上。这会导致模型需要同时适应任务的变化和图像大小的变化。
在 YOLOv2 中，预训练好的分类模型先在输入大小为 <span
class="math inline">\(448\times 448\)</span> 的图像分类任务上 finetune
10 个 epoch，finetune
后的模型用来对检测任务初始化。这样模型可以先适应高分辨率的图像，再迁移到同样分辨率的物体检测任务上。</p>
<h4 id="convolutional-with-anchor-boxes">Convolutional With Anchor
Boxes</h4>
<p>借鉴了 R-CNN 系列的工作，将 anchor 引入模型的设计。网络由 YOLO
的对每个网格预测两组检测框和一个类别向量，改为对每个 anchor
分别预测位置和类别向量。 同时调整了输入图像大小，由 <span
class="math inline">\(448\times 448\)</span> 调整为 <span
class="math inline">\(416\times
416\)</span>。这主要是因为作者观察发现有很多大物体中心落在图像中心，YOLOv2
的步长为 <span class="math inline">\(32\)</span>，调整后 feature map
的大小为 <span class="math inline">\(13\times 13\)</span>
是奇数，能找到绝对的中心。</p>
<h4 id="dimension-clusters">Dimension Clusters</h4>
<p>用 k-means 选择 anchor 的设计，并且用 <span
class="math inline">\(IOU\)</span> 衡量 gt box 与聚类中心的距离。 <span
class="math display">\[
d(box, centroid)=1−IOU(box, centroid)
\]</span> anchor 数量越多意味着 gt box
与聚类中心越接近，训练会相对容易，也会取得更好的效果，在 YOLOv2
中为了平衡时间与性能，选取了 5 个 anchor。 论文里比较了采用聚类选择
anchor 和采用 Faster R-CNN 方法选择 anchor
的性能差异，可以看出用聚类产生的 anchor 对检测有比较明显的帮助。
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-07%2017.23.54.png" width="60%" height="60%" /></p>
<h4 id="direct-location-prediction">Direct location prediction</h4>
<p>之前设计的坐标预测方式为： <span class="math display">\[
\begin{align}
x&amp;=(t_x*w_a)+x_a\\
y&amp;=(t_y*h_a)+y_a\\
\end{align}
\]</span> 即：模型预测 box 与 anchor
中心的偏移相对值，并且根据这个相对值计算最终的 box 坐标。
但是该方法可能遇到的问题是，<span class="math inline">\(t_x\)</span> 与
<span class="math inline">\(t_y\)</span>
可能预测为任何值，这会导致预测的检测框可能在图像的任何位置，从而在训练开始时训练比较不稳定。因此
YOLOv2 采取了预测检测框位置相对于格子左上角偏移的方法，并且用 <span
class="math inline">\(sigmoid\)</span> 函数将偏移值归一化到 0、1 之间。
<span class="math display">\[
\begin{align}
b_x&amp;=\sigma(t_x)+c_x \\
b_y&amp;=\sigma(t_y)+c_y \\
b_w&amp;=p_we^{t_w} \\
b_h&amp;=p_he^{t_h} \\
Pr(Object)\times IOU(b,object)&amp;=\sigma(t_o)
\end{align}
\]</span>
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-07%2018.39.51.png" width="60%" height="60%" /></p>
<h4 id="fine-grained-features">Fine-Grained Features</h4>
<p>从多尺度检测方法借鉴的思路，利用高分辨率的 feature map
帮助小物体的检测。YOLOv2 直接将高分辨率的 feature map 调整一下 shape
和低分辨率的 feature map 拼接起来得到最终的
feature。例如在论文中，作者将 <span class="math inline">\(26\times
26\times 512\)</span> 的 feature 调整为 <span
class="math inline">\(13\times 13\times 2048\)</span>。</p>
<h4 id="multi-scale-training">Multi-Scale Training</h4>
<p>每 10 个 epoch 随机选择一次训练图像尺寸，最小为 <span
class="math inline">\(320\times 320\)</span>，最大为 <span
class="math inline">\(608\times 608\)</span>。</p>
<p><font color="#d8d8d8">这里我有一点疑问，论文前面说选择 416 保证
feature map 长度是奇数，但是这里的设置并没管最终 feature map
尺寸的奇偶，所以这个是不是并不重要？</font></p>
<h4 id="实验结果">实验结果</h4>
<h5 id="每个-trick-带来的提升">每个 trick 带来的提升</h5>
<p><img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2011.06.46.png"
alt="截屏2023-10-08 11.06.46.png" /> ##### 验证 YOLOv2
在同样的性能下速度最快
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2011.28.03.png" width="50%" height="50%" />
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2013.10.48.png" width="50%" height="50%" />
##### 在 VOC 2012 和 COCO 上与其他算法比较 <img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2013.22.48.png"
alt="截屏2023-10-08 13.22.48.png" /></p>
<hr />
<h3 id="速度优化">速度优化</h3>
<p>提出了一个需要更少运算量，且在分类任务上与 VGG16 结果相当的网络。
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2014.05.53.png" width="50%" height="50%" />
*** ### 检测与分类共同训练 #### 基本思路
将检测和分类数据一起用来训练，对于检测，按照 YOLOv2
的正常流程训练；对于分类数据，只训练分类 loss。</p>
<h4 id="遇到的问题">遇到的问题</h4>
<p>分类模型最终一层通常采用 softmax 结构，这需要所有标签是互斥的，但
COCO 和 ImageNet 两个数据集里的标签通常有一定包含关系，例如：COCO
里标签是狗，ImageNet 里是猎犬，因此直接用 softmax
是不行的。如果按照多标签来处理，就放弃了一些本来的信息，比如 ImageNet
的标签之间是互斥的。</p>
<h4 id="解决方案">解决方案</h4>
<p>采用分级标签，模型在每一级标签上用 softmax
处理预测概率，最终将每一级概率相乘得到最终结果。
<img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-08%2015.06.35.png" width="50%" height="50%" /></p>
<p><font color="#d8d8d8">这篇论文的写作有点糟糕，很多公式里面的符号都没有清晰的解释，很多应该讲的内容和实验也不是很全，更像一个对于
YOLO 的技术补充。</font></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>论文解读</category>
        <category>Object Detection</category>
        <category>YOLO系列</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>YOLO</tag>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv3</title>
    <url>/2023/10/12/YOLOv3/</url>
    <content><![CDATA[<h3 id="论文信息">论文信息</h3>
<p>标题：YOLOv3: An Incremental Improvement<br />
作者：Joseph Redmon, Ali Farhadi<br />
单位：University of Washington<br />
论文链接：<a
href="https://arxiv.org/pdf/1804.02767.pdf">1804.02767.pdf</a><br />
项目链接：<a href="https://pjreddie.com/yolo/">YOLO: Real-Time Object
Detection</a></p>
<p><font color="#d8d8d8">准确来说这只是一篇技术报告，作者延续了 YOLOv2
的写作风格，非常随心所欲，对于阅读论文的人来说就会比较痛苦，强烈怀疑 v1
版本中间两个作者负责了主要的 writing 部分。</font></p>
<hr />
<h3 id="本文工作">本文工作</h3>
<p>在上一版本的基础上，更换了模型结构，并进行了一些其他调整，使得模型在保持实时性的基础上，进一步提升了检测效果。</p>
<hr />
<h3 id="method">Method</h3>
<h4 id="模型整体结构">模型整体结构</h4>
<p>论文作者并没有给出清晰易读的模型结构图，我在网络上看到了几个不同作者画的模型结构图，最早的应该是
<a href="https://blog.csdn.net/leviopku/article/details/82660381">CSDN
上木盏给出的结构图</a>，但木盏大佬明确要求了转载需要取得作者的书面同意，但是我看了下评论，作者已经很久没回复评论里的申请了。因此引用了知乎作者江大白根据木盏给出的结构图，重新绘制的结构图（<a
href="https://zhuanlan.zhihu.com/p/143747206">深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解
- 知乎</a>）。感谢对代码深入阅读的大佬们给出的结构图。</p>
<figure>
<img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/yolov3_1.png"
alt="yolov3_1.png" />
<figcaption aria-hidden="true">yolov3_1.png</figcaption>
</figure>
<p>从这个结构图可以看出 YOLOv3 相比于 YOLOv2 引入了类似 FPN
的多尺度结构，在不同尺度的 feature map 上分别进行检测。</p>
<h4 id="其他调整">其他调整</h4>
<h5 id="bounding-box-prediction">Bounding Box Prediction</h5>
<p>检测框的位置计算与 YOLOv2
的设置相同，但在置信度计算上，没有采用之前的用模型预测 IOU
的方式，而是采用了更常见也更简单的直接预测 0 或 1 的方式，并用 BCELoss
计算 loss。在 box prior 为与 gt 最接近的预设框时，label 为 1；如果 iou
大于阈值，但并不是最接近的，忽略；其余情况，label 为 0。YOLO
在训练时给每个 gt 仅分配一个预设框，希望最接近的框能检测到物体。</p>
<h5 id="class-prediction">Class Prediction</h5>
<p>对每一类分别进行二分类，而非在所有类别上进行多分类。这主要是为了可拓展性，部分数据集的类别之间不是互斥的，多分类的方法并不适用。</p>
<hr />
<h3 id="实验结果">实验结果</h3>
<h4 id="速度-准确性-trade-off">速度-准确性 trade off</h4>
<p>可以看到在差不多效果的情况下，YOLOv3 的速度优于 RetinaNet。 <img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-13%2015.00.47.png"
alt="截屏2023-10-13 15.00.47.png" /></p>
<h4 id="准确性">准确性</h4>
<p>单纯比较 AP 的话，YOLOv3 与最好的 RetinaNet
比还是有一定差距。但可以看到这个差距在 <span
class="math inline">\(AP_{50}\)</span> 时远低于 <span
class="math inline">\(AP_{75}\)</span>，说明 YOLOv3 与 YOLOv1
有相同的问题，可以检出但定位可能有偏差。<font color="#d8d8d8">这也意味着在实际应用上，如果任务是希望对检测物体有提示作用，并且比较速度，YOLO
系列的算法是一个很好的选择，但在对精度要求更高的任务上则需要准确率更高的算法。</font>
<img
src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2023-10-13%2016.28.29.png"
alt="截屏2023-10-13 16.28.29.png" /></p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>论文解读</category>
        <category>Object Detection</category>
        <category>YOLO系列</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>YOLO</tag>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>玻璃之锤</title>
    <url>/2023/10/21/%E7%8E%BB%E7%92%83%E4%B9%8B%E9%94%A4/</url>
    <content><![CDATA[<h3 id="作品简介">作品简介</h3>
<p>作者：贵志祐介<br />
出版时间：2004 年<br />
获奖情况：第 58 届日本推理作家协会奖<br />
侦探：榎本径<br />
类型：密室<br />
豆瓣评分：8.0</p>
<h3 id="内容介绍">内容介绍</h3>
<p>护理公司社长在办公室内死亡，办公室可能的入口只有防弹玻璃、被监控监视的大门、与专务及副社长办公室相连的门以及通风井。警方在调查后认为在办公室休息的专务是凶手，而作为代理律师的青砥纯子为了证明委托人的清白，与作为小偷兼防盗顾问的榎本径联手探索了密室的各种可能后找到了真凶。</p>
<h3 id="个人评价">个人评价</h3>
<h4 id="诡计">诡计</h4>
<p>作为真解答的诡计相当简洁质朴，但我觉得还是很难想到的，因为需要一定的专业知识，绝大部分人不具备那些知识，不会往那个方面去想。读完整个解谜部分回头看的话，会发现前面是对谜底有提示的，但真的很容易错过。<br />
另外，个人觉得诡计里比较出彩的地方是多重伪解答，重重伪解答充分利用了该公司研发的护理猴、护理机器人的特点，公司房间布局的特点以及监控摄像机的特点，并且能与前面的部分线索对上。<br />
但是个人看来不足之处有两点，一个是不太清楚最后真解答的诡计在现场勘查时是否能真的躲过警察；另一个是之前说的，因为需要专业知识，对读者有些许不公平。这可能也是推理小说，或者说本格推理遇到的一些困难，即：越来越多的手法被写过的情况下，新的小说既需要避开或更好地包装前人用过的手法，又需要考虑越来越先进的刑侦技术。</p>
<p>个人评分 ：8.0</p>
<h4 id="写作">写作</h4>
<p>采用了侦探、凶手双视角的写作方法，可以把侦探视角结束作为线索齐全展开推理。读的过程整体感受还是挺好的，不会觉得难读，也不会因为出场人物太多记不住人。尤其是侦探视角的时候，一层一层解答层层推进，很爽快。<br />
唯一一点点的不足是双视角切换略有些突兀，刚读第二部分的时候仿佛看了另一个故事。</p>
<p>个人评分 ：8.5</p>
<h4 id="动机">动机</h4>
<p>这本书的动机和作者最后的讨论其实有点社会派。当一个人被由社会土壤孕育出的黑暗伤害，这种伤害可能会持续很久，而为了像个正常人一样生活，在看到机会时原本的受害者可能铤而走险变成加害者去伤害其他人。</p>
<p>个人评分：7.5</p>
<h4 id="其他">其他</h4>
<p>读完全书会发现标题确实挺合适的。: P
另外作者在写作的时候应该下了不少功夫调查资料，很多涉及到的领域描述非常仔细，在谢辞里面也感谢了来自不同领域的专家的指导，这点上还挺有诚意的。</p>
<h4 id="整体评价">整体评价</h4>
<p>如果是很重视公平性，希望自己能推理出诡计的读者，读这本书可能会有点点失望，因为这个手法用到的知识真的非常非常冷门。对于除了解答和推理不想看其他部分的读者，这本书可能需要跳章，因为凶手视角的部分比较社会派，有一些凶手的成长经历。<br />
对于其他读者来说，个人认为这本书还是本比较值得推荐的书，诡计部分层层推进，故事读起来也很流畅。</p>
<p>整体评分：8.0</p>
<div class='spoiler collapsed'>
    <div class='spoiler-title'>
        部分剧透，关于那个冷门的知识，慎点，最好看完侦探视角再点，否则很影响观感
    </div>
    <div class='spoiler-content'>
        <p>防弹玻璃窗不一定是固定在窗框上的，通过一定的专业处理可以使玻璃窗小幅度的前后移动。</p>

    </div>
</div>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>生活随笔</category>
        <category>读书笔记</category>
        <category>推理小说</category>
        <category>密室</category>
      </categories>
      <tags>
        <tag>推理小说</tag>
        <tag>密室</tag>
        <tag>贵志祐介</tag>
        <tag>日本推理作家协会奖</tag>
      </tags>
  </entry>
  <entry>
    <title>2023花滑gpf观赛repo</title>
    <url>/2023/12/11/2023%E8%8A%B1%E6%BB%91gpf%E8%A7%82%E8%B5%9Brepo/</url>
    <content><![CDATA[<h3 id="各项目印象">各项目印象</h3>
<p>四天看下来整体上看的还是比较开心的，基本每个项目都有给我留下印象的表演和选手。</p>
<h4 id="女单">女单</h4>
<p>青年组，小小猫和申智雅是之前就有了解过的，上园恋奈之前没有关注过，但是短节目是我比较喜欢的；成年组，sp
除了坂本之外发挥都不太理想，但是到了 fp
大家发挥的都还不错，尤其是吉田阳菜 3A 挑战成功，比较可惜的是住吉丽音 4T
跳空。</p>
<h4 id="男单">男单</h4>
<p>青年组，只有中田璃士 fp
逆转成功印象比较深刻；成年组，不管小将还是老将基本都很拼，印象比较深刻的就是马琳琳的
4A 和 64 挑战，三浦佳生在身体明显出问题的情况下坚持完成了的自由滑以及
AYMOZ 莫名其妙的一连串跳空。</p>
<h4 id="双人">双人</h4>
<p>成年组的双人没留下什么深刻印象；青年组里，格鲁吉亚小双的表现一骑绝尘，尤其第一天青年双人滑被排到了最后一场比赛，前面看得已经有点困了，格鲁吉亚小双重新炒起了气氛。</p>
<h4 id="冰舞">冰舞</h4>
<p>青年组冰舞没什么印象；成年组里，印象比较深刻的反而是第四名的英国组合，运动风的节目很有特色。</p>
<h4 id="gala">Gala</h4>
<p>整体上双人组的表演滑比单人更有趣。有的组合表演了带有剧情的节目，有的组合表演了正赛上没有办法表演的动作，比如男伴头顶女伴的旋转。</p>
<p>单人里面，印象比较深的就是萧因法的空翻、萧因法和阳菜选到了同样的主题、AYMOZ
的步法大放送以及马琳琳的 ending pose。</p>
<h3 id="现场观赛感受">现场观赛感受</h3>
<h4 id="优点">优点</h4>
<p>现场的互动氛围非常好。不管是选手的谢场、在 K&amp;C
区与观众的互动还是颁奖后的绕场、gala
结束后的绕场及互动，选手做得都比较到位，很积极地和观众互动，调动气氛。这方面不到现场观赛是没办法感受到的。</p>
<p>另外，现场确实更容易被带入节目的氛围。作为花织粉，还是第一次沉浸到花织的表演滑里面。</p>
<h4 id="缺点">缺点</h4>
<p>没有提前告知一些注意事项，比如现场不允许挥旗子，观众以及选手都不知道。</p>
<p>另外，现场的翻译很多东西没有翻译到位。比如美国冰舞组合在说到他们未来计划的时候，先说了他们打算
take a
vacation，带入语境感觉是个小玩笑，然后才说了他们未来的比赛计划，打算参加
4cc，但是翻译的时候前面说的就都没了只剩下 4cc
了。对坂本花织和马琳琳的采访，也是选手说了很多，翻译只翻译了一点点。</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>生活随笔</category>
        <category>观赛体验</category>
        <category>花样滑冰</category>
      </categories>
      <tags>
        <tag>花样滑冰</tag>
      </tags>
  </entry>
  <entry>
    <title>Qwen-VL</title>
    <url>/2024/01/04/Qwen-VL/</url>
    <content><![CDATA[<h3 id="Qwen-VL-与-Qwen-LM-相比的改进"><a href="#Qwen-VL-与-Qwen-LM-相比的改进" class="headerlink" title="Qwen-VL 与 Qwen-LM 相比的改进"></a>Qwen-VL 与 Qwen-LM 相比的改进</h3><ul>
<li>Visual receptor</li>
<li>Input-output interface</li>
<li>3-stage training pipeline</li>
<li>Multilingual multimodal cleaned corpus</li>
</ul>
<hr>
<h3 id="模型构成"><a href="#模型构成" class="headerlink" title="模型构成"></a>模型构成</h3><ul>
<li>LLM: Qwen-7 B</li>
<li>Visual Encoder: ViT</li>
<li>VL Adapter: single-layer cross-attention module initialized randomly<ul>
<li>A group of trainable vectors (Embeddings) as query vectors</li>
<li>The image features from the visual encoder as keys</li>
<li>This mechanism compresses the visual feature sequence to a fixed length of 256</li>
<li>2 D absolute positional encodings are incorporated into the cross-attention mechanism’s query-key pairs</li>
<li>The compressed image feature sequence of length 256 is subsequently fed into the large language model</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Qwen-LM-的输入"><a href="#Qwen-LM-的输入" class="headerlink" title="Qwen LM 的输入"></a>Qwen LM 的输入</h3><ul>
<li>图像特征<ul>
<li>VL adapter 输出的特征，前后加上\<img\>和\&lt;/img>表明图像特征的开始和结束</li>
</ul>
</li>
<li>Bounding box<ul>
<li>本文希望模型能更细粒度地理解图像，因此设计了涉及 bounding box 的任务，例如框定一个区域由模型进行描述</li>
<li>训练时 bounding box 由字符串格式的 $(X_{topleft},Y_{topleft}),(X_{bottomright},Y_{bottomright})$ 提供，前后加上\<box\>和\&lt;/box></li>
<li>在文本中加入\<ref\>和\&lt;/ref>，表明 bounding box 对应的文本</li>
</ul>
</li>
</ul>
<hr>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li>训练流程<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-02%252018.29.42.png" alt="截屏2024-01-02 18.29.42.png"></li>
</ul>
</li>
<li>预训练<ul>
<li>仅训练 ViT 和 VL adapter</li>
<li>训练数据共 14 亿图像-文本对，英语数据占 77.3%，中文数据占 22.7%</li>
<li>The training objective is to minimize the cross-entropy of the text tokens. <font color="#a5a5a5">这个 loss 是什么意思</font></li>
</ul>
</li>
<li>多任务训练<ul>
<li>在 7 个任务上进行训练，模型的各个部分都参与训练<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-02%252019.00.20.png" alt="截屏2024-01-02 19.00.20.png"></li>
</ul>
</li>
<li>图像尺寸由 $224\times 224$ 调整到 $448\times 448$</li>
</ul>
</li>
<li>Supervised Fine-tuning<ul>
<li>优化对话能力，fine-tune 后的结果是 Qwen-VL-Chat</li>
</ul>
</li>
</ul>
<hr>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ul>
<li>Image Caption and General Visual Question Answering<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-03%252013.44.23.png" alt="截屏2024-01-03 13.44.23.png"></li>
</ul>
</li>
<li>Text-oriented Visual Question Answering<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-03%252013.47.56.png" alt="截屏2024-01-03 13.47.56.png"></li>
</ul>
</li>
<li>Refer Expression Comprehension<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-03%252013.50.18.png" alt="截屏2024-01-03 13.50.18.png"></li>
</ul>
</li>
<li>Few-shot Learning on Vision-Language Tasks<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-03%252013.52.41.png" alt="截屏2024-01-03 13.52.41.png"></li>
</ul>
</li>
<li>Instruction Following in Real-world User Behavior<ul>
<li><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%25E6%2588%25AA%25E5%25B1%258F2024-01-03%252013.54.40.png" alt="截屏2024-01-03 13.54.40.png"></li>
</ul>
</li>
</ul>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>论文解读</category>
        <category>大模型</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>大模型</tag>
      </tags>
  </entry>
  <entry>
    <title>DETR</title>
    <url>/2024/01/06/DETR/</url>
    <content><![CDATA[<h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>设计一种基于 transformer 的物体检测框架，避免基于 CNN 方法的 NMS、anchor 等需要手工设计的环节，并且取得与 CNN 方法接近或更好的效果。</p>
<hr>
<h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><h4 id="模型整体结构"><a href="#模型整体结构" class="headerlink" title="模型整体结构"></a>模型整体结构</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-05%2010.21.07.png" alt="截屏2024-01-05 10.21.07.png"></p>
<h4 id="Loss-计算"><a href="#Loss-计算" class="headerlink" title="Loss 计算"></a>Loss 计算</h4><h5 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h5><ol>
<li>用匈牙利算法找到预测框与标注框之间的匹配关系：$L_{match}(y_i,\hat{y}_{\sigma(i)})=-\mathbb{1}_{c_i\neq\phi}\hat{p}_{\sigma(i)}(c_i)+\mathbb{1}_{c_i\neq\phi}L_{box}(b_i, \hat{b}_{\sigma(i)})$</li>
<li>根据匹配后的 bounding box 对，计算 loss：</li>
</ol>
<script type="math/tex; mode=display">
\begin{align*}
L_{Hungarian(y,\hat{y})}&=\sum_{i=1}^N[-\log{\hat{p}_{\hat{\sigma}_{i}}}(c_i)+\mathbb{1}_{c_i\neq\phi}L_{box}(b_i, \hat{b}_{\hat\sigma(i)})] \\
L_{box}(b_i, \hat{b}_{\sigma(i)})&=\lambda_{iou}L_{iou}(b_i, \hat{b}_{\sigma(i)})+\lambda_{L1}||b_i-\hat{b}_{\sigma(i)}||_1\\
\end{align*}</script><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><h5 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h5><ol>
<li>对 CNN 输出的 feature map 降维</li>
<li>把 feature map 的 HW 维展平，并加入 position embedding</li>
<li>将处理后的特征经过由 multi-head self-attention 和 FFN 组成的 encoder 模块</li>
</ol>
<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><h5 id="主要步骤-1"><a href="#主要步骤-1" class="headerlink" title="主要步骤"></a>主要步骤</h5><ol>
<li>Query 之间进行 self attention</li>
<li>Query 作为 q，图像特征作为 k，v 进行 multi-head attention</li>
</ol>
<hr>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h4><p>COCO 2017</p>
<pre><code>- 118 k 训练数据，5 k 验证集
- 每张图像都有 bounding box 和全景分割标注
- 平均每张训练图像由 7 个 instance，最多的图像有 63 个
</code></pre><h4 id="与-Faster-R-CNN-比较"><a href="#与-Faster-R-CNN-比较" class="headerlink" title="与 Faster R-CNN 比较"></a>与 Faster R-CNN 比较</h4><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-06%2016.13.51.png" alt="截屏2024-01-06 16.13.51.png"></p>
<ol>
<li>与 pytorch 自带的 faster r-cnn 相比，优化后的通常会有 1～2 个点的提升</li>
<li>在 fps 相近的情况下，detr 的效果与 faster r-cnn 优化后的效果相当</li>
<li>Detr 在大物体上的检出效果明显更好，反之在小物体上效果较差</li>
</ol>
<h4 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h4><h5 id="Encoder-层的数量"><a href="#Encoder-层的数量" class="headerlink" title="Encoder 层的数量"></a>Encoder 层的数量</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-06%2016.20.34.png" alt="截屏2024-01-06 16.20.34.png"></p>
<ol>
<li>Encoder 层对物体检出有比较重要的作用，尤其对于大物体检出</li>
<li>增加 encoder 层，检出效果会随之提升</li>
</ol>
<h5 id="Decoder-层的数量"><a href="#Decoder-层的数量" class="headerlink" title="Decoder 层的数量"></a>Decoder 层的数量</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-06%2016.26.32.png" alt="截屏2024-01-06 16.26.32.png|400"></p>
<ol>
<li>Decoder 层数增加，检出效果会提升</li>
<li>NMS 在仅有 1 层 decoder 的时候能带来显著提升，decoder 层数增加后，NMS 几乎没有效果</li>
</ol>
<h5 id="FFN-的效果"><a href="#FFN-的效果" class="headerlink" title="FFN 的效果"></a>FFN 的效果</h5><p>删掉 FFN 后，AP 下降了 2.3</p>
<h5 id="Position-embedding-的效果"><a href="#Position-embedding-的效果" class="headerlink" title="Position embedding 的效果"></a>Position embedding 的效果</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-06%2016.49.41.png" alt="截屏2024-01-06 16.49.41.png"></p>
<h5 id="不同-loss-的效果"><a href="#不同-loss-的效果" class="headerlink" title="不同 loss 的效果"></a>不同 loss 的效果</h5><p><img src="https://yumiwawa-1300546587.cos.ap-beijing.myqcloud.com/Obsidian/%E6%88%AA%E5%B1%8F2024-01-06%2016.54.14.png" alt="截屏2024-01-06 16.54.14.png"></p>
<hr>
<h3 id="个人总结"><a href="#个人总结" class="headerlink" title="个人总结"></a>个人总结</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol>
<li>需要手工设计的模块较少，不需要后处理</li>
<li>检测效果与 cnn 模型相当，且在大物体上效果突出</li>
</ol>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ol>
<li>小物体检测效果不足</li>
<li>训练时间更长</li>
</ol>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>论文解读</category>
        <category>Object Detection</category>
        <category>DETR</category>
      </categories>
      <tags>
        <tag>论文解读</tag>
        <tag>DETR</tag>
        <tag>ObjectDetection</tag>
      </tags>
  </entry>
  <entry>
    <title>夏与冬的奏鸣曲</title>
    <url>/2024/01/06/%E5%A4%8F%E4%B8%8E%E5%86%AC%E7%9A%84%E5%A5%8F%E9%B8%A3%E6%9B%B2/</url>
    <content><![CDATA[<h3 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a>内容介绍</h3><p>如月乌有接到主编委托，与桐璃一同到小岛上采访几个曾经在岛上生活过一段时间，并于近期重聚的人。岛上发生了多起命案，在探索真相的同时，几人之间过去的纠葛逐渐展现。最终乌有发现，自己与桐璃早就置身事中，无法独善其身。</p>
<h3 id="个人评价"><a href="#个人评价" class="headerlink" title="个人评价"></a>个人评价</h3><h4 id="诡计"><a href="#诡计" class="headerlink" title="诡计"></a>诡计</h4><p>这本书几乎没有什么称得上诡计的地方。虽然故事的背景非常本格，有暴风雪山庄、雪地密室这样很适合展开本格推理的设定，但是解答非常不本格。<br>虽然关于凶手身份，可能算是有一点点诡计在，但由于文中对于推理凶手身份的描写几乎没有，因此，在揭露的时候也没有感受到强烈的震撼。</p>
<p>个人评分：2.0</p>
<h4 id="故事"><a href="#故事" class="headerlink" title="故事"></a>故事</h4><p>抛开推理的部分，整体的故事我还是比较喜欢的。虽然中间插入了大段大段的关于立体主义的“炫学”，而且我没有完全看懂，但是我莫名地并不讨厌这些描写。而且“炫学”的部分与整个故事的背景结合地也算比较好。结尾部分留下了充足的遐想和讨论的空间。<br>但是，看完书之后去看豆瓣的长评，发现这本书在翻译成中文的时候出现了很严重的翻译问题，导致看新星版中译本理解的结尾与日文原意可能有比较严重的出入。这导致阅读体验有很大偏差。</p>
<p>个人评分：6.5</p>
<h4 id="人物"><a href="#人物" class="headerlink" title="人物"></a>人物</h4><p>大部分推理小说看完我对人物印象都不是很深刻，但是这本书看完我还是很喜欢桐璃这个人物的。虽然有少女的任性、调皮、不听劝的一面，但是在发生重大变故的时候，迅速调整了心态，反过来安慰乌有的桐璃谁能不爱呢。</p>
<p>个人评分：7.5</p>
<h4 id="整体评价"><a href="#整体评价" class="headerlink" title="整体评价"></a>整体评价</h4><p>个人认为这本书是一本故事性远大于推理性的书。如果想要看酷炫的手法、逆天的诡计，这本书里都是没有的。但如果把这本书当作一本有些悬疑性的小说，那阅读体验应该还不错。<br>个人评价低的原因是：</p>
<ol>
<li>这本书是以推理小说被安利给我的，我原本期待看到一些手法上的设计</li>
<li>翻译出现了问题</li>
</ol>
<p>整体评价：5.0</p>
<link rel="stylesheet" href="/css/spoiler.css" type="text/css"><script src="/js/spoiler.js" type="text/javascript" async></script>]]></content>
      <categories>
        <category>生活随笔</category>
        <category>读书笔记</category>
        <category>推理小说</category>
      </categories>
      <tags>
        <tag>推理小说</tag>
        <tag>麻耶雄嵩</tag>
      </tags>
  </entry>
</search>
